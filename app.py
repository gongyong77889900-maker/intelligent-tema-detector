import streamlit as st
import pandas as pd
import numpy as np
import io
import re
import logging
from collections import defaultdict
from datetime import datetime
from itertools import combinations
import warnings
import traceback

# é…ç½®æ—¥å¿—å’Œè­¦å‘Š
warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger('MultiAccountWashTrade')

# Streamlité¡µé¢é…ç½®
st.set_page_config(
    page_title="æ™ºèƒ½å¤šè´¦æˆ·å¯¹åˆ·æ£€æµ‹ç³»ç»Ÿ",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded"
)

class Config:
    """é…ç½®å‚æ•°ç±»"""
    def __init__(self):
        self.min_amount = 10
        self.amount_similarity_threshold = 0.9
        self.min_continuous_periods = 3
        self.max_accounts_in_group = 5
        self.supported_file_types = ['.xlsx', '.xls', '.csv']

class WashTradeDetector:
    def __init__(self, config=None):
        self.config = config or Config()
        self.data_processed = False
        self.df_valid = None
        self.export_data = []
        self.performance_stats = {}
    
    def upload_and_process(self, uploaded_file):
        """ä¸Šä¼ å¹¶å¤„ç†æ–‡ä»¶"""
        try:
            if uploaded_file is None:
                st.error("âŒ æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶")
                return None, None
            
            filename = uploaded_file.name
            logger.info(f"âœ… å·²ä¸Šä¼ æ–‡ä»¶: {filename}")
            
            if not any(filename.endswith(ext) for ext in self.config.supported_file_types):
                st.error(f"âŒ ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {filename}")
                return None, None
            
            if filename.endswith('.csv'):
                df = pd.read_csv(uploaded_file, encoding='utf-8')
            else:
                df = pd.read_excel(uploaded_file)
            
            logger.info(f"åŸå§‹æ•°æ®ç»´åº¦: {df.shape}")
            
            return df, filename
            
        except Exception as e:
            logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
            st.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {str(e)}")
            return None, None
    
    def auto_detect_columns(self, df):
        """è‡ªåŠ¨æ£€æµ‹åˆ—å"""
        column_mapping = {}
        
        # æ˜¾ç¤ºåŸå§‹åˆ—åä¾›å‚è€ƒ
        st.write("ğŸ“‹ åŸå§‹æ–‡ä»¶åˆ—å:", df.columns.tolist())
        
        # è‡ªåŠ¨æ£€æµ‹é€»è¾‘
        for col in df.columns:
            col_lower = str(col).lower()
            
            # æ£€æµ‹ä¼šå‘˜è´¦å·åˆ—
            if any(keyword in col_lower for keyword in ['ä¼šå‘˜', 'è´¦å·', 'è´¦æˆ·', 'ç”¨æˆ·']):
                if 'ä¼šå‘˜è´¦å·' not in column_mapping.values():
                    column_mapping[col] = 'ä¼šå‘˜è´¦å·'
            
            # æ£€æµ‹æœŸå·åˆ—
            elif any(keyword in col_lower for keyword in ['æœŸå·', 'æœŸæ•°', 'æœŸæ¬¡', 'æœŸ']):
                if 'æœŸå·' not in column_mapping.values():
                    column_mapping[col] = 'æœŸå·'
            
            # æ£€æµ‹å†…å®¹åˆ—
            elif any(keyword in col_lower for keyword in ['å†…å®¹', 'æŠ•æ³¨', 'ä¸‹æ³¨']):
                if 'å†…å®¹' not in column_mapping.values():
                    column_mapping[col] = 'å†…å®¹'
            
            # æ£€æµ‹é‡‘é¢åˆ—
            elif any(keyword in col_lower for keyword in ['é‡‘é¢', 'ä¸‹æ³¨æ€»é¢', 'æŠ•æ³¨é‡‘é¢', 'æ€»é¢']):
                if 'é‡‘é¢' not in column_mapping.values():
                    column_mapping[col] = 'é‡‘é¢'
            
            # æ£€æµ‹å½©ç§åˆ—
            elif any(keyword in col_lower for keyword in ['å½©ç§', 'å½©ç¥¨', 'æ¸¸æˆ']):
                if 'å½©ç§' not in column_mapping.values():
                    column_mapping[col] = 'å½©ç§'
        
        return column_mapping
    
    def parse_column_data(self, df):
        """è§£æåˆ—ç»“æ„æ•°æ® - ç®€åŒ–ç‰ˆæœ¬"""
        try:
            # è‡ªåŠ¨æ£€æµ‹åˆ—å
            column_mapping = self.auto_detect_columns(df)
            
            if not column_mapping:
                st.error("âŒ æ— æ³•è‡ªåŠ¨è¯†åˆ«åˆ—åï¼Œè¯·æ‰‹åŠ¨æŒ‡å®šåˆ—åæ˜ å°„")
                return pd.DataFrame()
            
            st.success("âœ… è‡ªåŠ¨è¯†åˆ«åˆ°ä»¥ä¸‹åˆ—åæ˜ å°„:")
            for orig, mapped in column_mapping.items():
                st.write(f"  {orig} â†’ {mapped}")
            
            # é‡å‘½ååˆ—
            df_renamed = df.rename(columns=column_mapping)
            
            # æ£€æŸ¥å¿…è¦åˆ—
            required_cols = ['ä¼šå‘˜è´¦å·', 'æœŸå·', 'å†…å®¹', 'é‡‘é¢']
            missing_cols = [col for col in required_cols if col not in df_renamed.columns]
            
            if missing_cols:
                st.error(f"âŒ ç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
                return pd.DataFrame()
            
            # æ·»åŠ å½©ç§åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if 'å½©ç§' not in df_renamed.columns:
                df_renamed['å½©ç§'] = 'æœªçŸ¥å½©ç§'
            
            # é€‰æ‹©éœ€è¦çš„åˆ—
            df_clean = df_renamed[['ä¼šå‘˜è´¦å·', 'æœŸå·', 'å†…å®¹', 'é‡‘é¢', 'å½©ç§']].copy()
            df_clean = df_clean.dropna(subset=['ä¼šå‘˜è´¦å·', 'æœŸå·', 'å†…å®¹', 'é‡‘é¢'])
            
            # æ•°æ®æ¸…ç†
            for col in ['ä¼šå‘˜è´¦å·', 'æœŸå·', 'å†…å®¹', 'å½©ç§']:
                df_clean[col] = df_clean[col].astype(str).str.strip()
            
            # æå–é‡‘é¢å’Œæ–¹å‘
            df_clean['æŠ•æ³¨é‡‘é¢'] = df_clean['é‡‘é¢'].apply(self.extract_bet_amount_safe)
            df_clean['æŠ•æ³¨æ–¹å‘'] = df_clean['å†…å®¹'].apply(self.extract_direction_from_content)
            
            # è¿‡æ»¤æœ‰æ•ˆè®°å½•
            df_valid = df_clean[
                (df_clean['æŠ•æ³¨æ–¹å‘'] != '') & 
                (df_clean['æŠ•æ³¨é‡‘é¢'] >= self.config.min_amount)
            ].copy()
            
            if len(df_valid) == 0:
                st.error("âŒ è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆè®°å½•")
                return pd.DataFrame()
            
            # æ˜¾ç¤ºæ•°æ®æ¦‚è§ˆ
            with st.expander("ğŸ“Š æ•°æ®æ¦‚è§ˆ", expanded=False):
                st.write(f"æ€»è®°å½•æ•°: {len(df_clean)}")
                st.write(f"æœ‰æ•ˆè®°å½•æ•°: {len(df_valid)}")
                st.write(f"å”¯ä¸€æœŸå·æ•°: {df_valid['æœŸå·'].nunique()}")
                st.write(f"å”¯ä¸€è´¦æˆ·æ•°: {df_valid['ä¼šå‘˜è´¦å·'].nunique()}")
                
                lottery_stats = df_valid['å½©ç§'].value_counts()
                st.write(f"å½©ç§åˆ†å¸ƒ: {dict(lottery_stats)}")
                
                direction_stats = df_valid['æŠ•æ³¨æ–¹å‘'].value_counts()
                st.write(f"æŠ•æ³¨æ–¹å‘åˆ†å¸ƒ: {dict(direction_stats)}")
            
            self.data_processed = True
            self.df_valid = df_valid
            return df_valid
            
        except Exception as e:
            logger.error(f"æ•°æ®è§£æå¤±è´¥: {str(e)}")
            st.error(f"æ•°æ®è§£æå¤±è´¥: {str(e)}")
            st.error(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return pd.DataFrame()
    
    def extract_bet_amount_safe(self, amount_text):
        """å®‰å…¨æå–æŠ•æ³¨é‡‘é¢"""
        try:
            if pd.isna(amount_text):
                return 0
            
            text = str(amount_text).strip()
            
            # ç›´æ¥å°è¯•è½¬æ¢
            try:
                cleaned_text = text.replace(',', '').replace('ï¼Œ', '').replace(' ', '')
                amount = float(cleaned_text)
                if amount >= self.config.min_amount:
                    return amount
            except:
                pass
            
            # å°è¯•æå–æ•°å­—
            numbers = re.findall(r'\d+\.?\d*', text)
            if numbers:
                try:
                    amount = float(numbers[0])
                    if amount >= self.config.min_amount:
                        return amount
                except:
                    pass
            
            return 0
            
        except Exception as e:
            return 0
    
    def extract_direction_from_content(self, content):
        """ä»å†…å®¹åˆ—æå–æŠ•æ³¨æ–¹å‘"""
        try:
            if pd.isna(content):
                return ""
            
            content_str = str(content).strip().lower()
            
            direction_patterns = {
                'å°': ['å°', 'small', 'xia'],
                'å¤§': ['å¤§', 'big', 'da'], 
                'å•': ['å•', 'odd', 'dan'],
                'åŒ': ['åŒ', 'even', 'shuang'],
                'é¾™': ['é¾™', 'long', 'dragon'],
                'è™': ['è™', 'hu', 'tiger']
            }
            
            for direction, patterns in direction_patterns.items():
                for pattern in patterns:
                    if pattern in content_str:
                        return direction
            
            return ""
        except Exception as e:
            return ""
    
    def detect_all_wash_trades(self):
        """æ£€æµ‹æ‰€æœ‰ç±»å‹çš„å¯¹åˆ·äº¤æ˜“"""
        if not self.data_processed or self.df_valid is None or len(self.df_valid) == 0:
            st.error("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯ç”¨äºæ£€æµ‹")
            return []
        
        self.performance_stats = {
            'start_time': datetime.now(),
            'total_records': len(self.df_valid),
            'total_periods': self.df_valid['æœŸå·'].nunique(),
            'total_accounts': self.df_valid['ä¼šå‘˜è´¦å·'].nunique()
        }
        
        # æ’é™¤åŒä¸€è´¦æˆ·å¤šæ–¹å‘ä¸‹æ³¨
        multi_direction_mask = (
            self.df_valid.groupby(['æœŸå·', 'ä¼šå‘˜è´¦å·'])['æŠ•æ³¨æ–¹å‘']
            .transform('nunique') > 1
        )
        df_filtered = self.df_valid[~multi_direction_mask].copy()
        
        if len(df_filtered) == 0:
            st.error("âŒ è¿‡æ»¤åæ— æœ‰æ•ˆæ•°æ®")
            return []
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        all_patterns = []
        
        # æ£€æµ‹2-3ä¸ªè´¦æˆ·çš„å¯¹åˆ·æ¨¡å¼
        for account_count in range(2, 4):
            status_text.text(f"ğŸ” æ£€æµ‹{account_count}ä¸ªè´¦æˆ·å¯¹åˆ·æ¨¡å¼...")
            patterns = self.detect_n_account_patterns(df_filtered, account_count)
            all_patterns.extend(patterns)
            
            progress = (account_count - 1) / 2
            progress_bar.progress(progress)
        
        progress_bar.progress(1.0)
        status_text.text("âœ… æ£€æµ‹å®Œæˆ")
        
        self.performance_stats['end_time'] = datetime.now()
        self.performance_stats['detection_time'] = (
            self.performance_stats['end_time'] - self.performance_stats['start_time']
        ).total_seconds()
        self.performance_stats['total_patterns'] = len(all_patterns)
        
        return all_patterns
    
    def detect_n_account_patterns(self, df_filtered, n_accounts):
        """æ£€æµ‹Nä¸ªè´¦æˆ·å¯¹åˆ·æ¨¡å¼"""
        wash_records = []
        
        period_groups = df_filtered.groupby(['æœŸå·', 'å½©ç§'])
        
        for period_key, period_data in period_groups:
            period_accounts = period_data['ä¼šå‘˜è´¦å·'].unique()
            
            if len(period_accounts) < n_accounts:
                continue
            
            # è·å–è´¦æˆ·ä¿¡æ¯
            account_info = {}
            for _, row in period_data.iterrows():
                account = row['ä¼šå‘˜è´¦å·']
                account_info[account] = {
                    'direction': row['æŠ•æ³¨æ–¹å‘'],
                    'amount': row['æŠ•æ³¨é‡‘é¢']
                }
            
            # æ£€æµ‹æ‰€æœ‰ç»„åˆ
            for account_group in combinations(period_accounts, n_accounts):
                # æ£€æŸ¥æ˜¯å¦å½¢æˆå¯¹ç«‹
                directions = [account_info[acc]['direction'] for acc in account_group]
                amounts = [account_info[acc]['amount'] for acc in account_group]
                
                # ç®€å•å¯¹ç«‹æ£€æµ‹ï¼šå¤§å°ã€å•åŒã€é¾™è™
                if self.is_opposite_directions(directions):
                    dir1_total = sum(amounts[i] for i, d in enumerate(directions) if d in ['å¤§', 'å•', 'é¾™'])
                    dir2_total = sum(amounts[i] for i, d in enumerate(directions) if d in ['å°', 'åŒ', 'è™'])
                    
                    if dir1_total > 0 and dir2_total > 0:
                        similarity = min(dir1_total, dir2_total) / max(dir1_total, dir2_total)
                        
                        if similarity >= self.config.amount_similarity_threshold:
                            record = {
                                'æœŸå·': period_data['æœŸå·'].iloc[0],
                                'å½©ç§': period_data['å½©ç§'].iloc[0],
                                'è´¦æˆ·ç»„': list(account_group),
                                'æ–¹å‘ç»„': directions,
                                'é‡‘é¢ç»„': amounts,
                                'æ€»é‡‘é¢': dir1_total + dir2_total,
                                'ç›¸ä¼¼åº¦': similarity,
                                'è´¦æˆ·æ•°é‡': n_accounts
                            }
                            wash_records.append(record)
        
        return self.find_continuous_patterns(wash_records)
    
    def is_opposite_directions(self, directions):
        """æ£€æŸ¥æ–¹å‘æ˜¯å¦å¯¹ç«‹"""
        opposites = [{'å¤§', 'å°'}, {'å•', 'åŒ'}, {'é¾™', 'è™'}]
        
        for opp_set in opposites:
            if set(directions) == opp_set:
                return True
        
        return False
    
    def find_continuous_patterns(self, wash_records):
        """æŸ¥æ‰¾è¿ç»­å¯¹åˆ·æ¨¡å¼"""
        if not wash_records:
            return []
        
        account_group_patterns = defaultdict(list)
        for record in wash_records:
            account_group_key = (tuple(sorted(record['è´¦æˆ·ç»„'])), record['å½©ç§'])
            account_group_patterns[account_group_key].append(record)
        
        continuous_patterns = []
        
        for (account_group, lottery), records in account_group_patterns.items():
            sorted_records = sorted(records, key=lambda x: x['æœŸå·'])
            
            if len(sorted_records) >= self.config.min_continuous_periods:
                total_investment = sum(r['æ€»é‡‘é¢'] for r in sorted_records)
                similarities = [r['ç›¸ä¼¼åº¦'] for r in sorted_records]
                avg_similarity = np.mean(similarities) if similarities else 0
                
                continuous_patterns.append({
                    'è´¦æˆ·ç»„': list(account_group),
                    'å½©ç§': lottery,
                    'è´¦æˆ·æ•°é‡': len(account_group),
                    'å¯¹åˆ·æœŸæ•°': len(sorted_records),
                    'æ€»æŠ•æ³¨é‡‘é¢': total_investment,
                    'å¹³å‡ç›¸ä¼¼åº¦': avg_similarity,
                    'è¯¦ç»†è®°å½•': sorted_records
                })
        
        return continuous_patterns
    
    def display_detailed_results(self, patterns):
        """æ˜¾ç¤ºè¯¦ç»†æ£€æµ‹ç»“æœ"""
        st.write("\n" + "="*60)
        st.write("ğŸ¯ å¤šè´¦æˆ·å¯¹åˆ·æ£€æµ‹ç»“æœ")
        st.write("="*60)
        
        if not patterns:
            st.error("âŒ æœªå‘ç°ç¬¦åˆé˜ˆå€¼æ¡ä»¶çš„è¿ç»­å¯¹åˆ·æ¨¡å¼")
            return
        
        for i, pattern in enumerate(patterns, 1):
            st.markdown(f"**å¯¹åˆ·ç»„ {i}:** {' â†” '.join(pattern['è´¦æˆ·ç»„'])}")
            st.markdown(f"**å½©ç§:** {pattern['å½©ç§']} | **å¯¹åˆ·æœŸæ•°:** {pattern['å¯¹åˆ·æœŸæ•°']}æœŸ")
            st.markdown(f"**æ€»é‡‘é¢:** {pattern['æ€»æŠ•æ³¨é‡‘é¢']:.2f}å…ƒ | **å¹³å‡åŒ¹é…:** {pattern['å¹³å‡ç›¸ä¼¼åº¦']:.2%}")
            
            st.markdown("**è¯¦ç»†è®°å½•:**")
            for j, record in enumerate(pattern['è¯¦ç»†è®°å½•'], 1):
                account_info = []
                for account, direction, amount in zip(record['è´¦æˆ·ç»„'], record['æ–¹å‘ç»„'], record['é‡‘é¢ç»„']):
                    account_info.append(f"{account}({direction}:{amount})")
                
                st.markdown(f"{j}. **æœŸå·:** {record['æœŸå·']} | **æ–¹å‘:** {' â†” '.join(account_info)} | **åŒ¹é…åº¦:** {record['ç›¸ä¼¼åº¦']:.2%}")
            
            if i < len(patterns):
                st.markdown("---")
    
    def export_to_excel(self, patterns, filename):
        """å¯¼å‡ºæ£€æµ‹ç»“æœåˆ°Excelæ–‡ä»¶"""
        if not patterns:
            st.error("âŒ æ²¡æœ‰å¯¹åˆ·æ•°æ®å¯å¯¼å‡º")
            return None, None
        
        export_data = []
        
        for group_idx, pattern in enumerate(patterns, 1):
            for record_idx, record in enumerate(pattern['è¯¦ç»†è®°å½•'], 1):
                account_directions = []
                for account, direction, amount in zip(record['è´¦æˆ·ç»„'], record['æ–¹å‘ç»„'], record['é‡‘é¢ç»„']):
                    account_directions.append(f"{account}({direction}:{amount})")
                
                export_data.append({
                    'å¯¹åˆ·ç»„ç¼–å·': group_idx,
                    'è´¦æˆ·ç»„': ' â†” '.join(pattern['è´¦æˆ·ç»„']),
                    'å½©ç§': pattern['å½©ç§'],
                    'è´¦æˆ·æ•°é‡': pattern['è´¦æˆ·æ•°é‡'],
                    'å¯¹åˆ·æœŸæ•°': pattern['å¯¹åˆ·æœŸæ•°'],
                    'æ€»æŠ•æ³¨é‡‘é¢': pattern['æ€»æŠ•æ³¨é‡‘é¢'],
                    'å¹³å‡ç›¸ä¼¼åº¦': f"{pattern['å¹³å‡ç›¸ä¼¼åº¦']:.2%}",
                    'æœŸå·': record['æœŸå·'],
                    'é‡‘é¢': record['æ€»é‡‘é¢'],
                    'åŒ¹é…åº¦': f"{record['ç›¸ä¼¼åº¦']:.2%}",
                    'è´¦æˆ·æ–¹å‘': ' | '.join(account_directions)
                })
        
        df_export = pd.DataFrame(export_data)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        export_filename = f"å¯¹åˆ·æ£€æµ‹æŠ¥å‘Š_{timestamp}.xlsx"
        
        try:
            output = io.BytesIO()
            with pd.ExcelWriter(output, engine='openpyxl') as writer:
                df_export.to_excel(writer, sheet_name='è¯¦ç»†è®°å½•', index=False)
            
            output.seek(0)
            st.success(f"âœ… ExcelæŠ¥å‘Šå·²ç”Ÿæˆ: {export_filename}")
            
            return output, export_filename
            
        except Exception as e:
            st.error(f"âŒ å¯¼å‡ºExcelå¤±è´¥: {str(e)}")
            return None, None

def run_simple_tema_analysis(df):
    """è¿è¡Œç®€åŒ–çš„ç‰¹ç åˆ†æ"""
    try:
        st.header("ğŸ¯ ç‰¹ç åˆ†æï¼ˆç®€åŒ–ç‰ˆï¼‰")
        
        # æ£€æŸ¥å¿…è¦åˆ—
        required_cols = ['ä¼šå‘˜è´¦å·', 'æœŸå·', 'å½©ç§', 'å†…å®¹']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            st.error(f"âŒ ç‰¹ç åˆ†æç¼ºå°‘å¿…è¦åˆ—: {missing_cols}")
            return
        
        # æ•°æ®æ¦‚è§ˆ
        st.write(f"ğŸ“Š æ€»æ•°æ®é‡: {len(df):,} è¡Œ")
        st.write(f"ğŸ‘¥ å”¯ä¸€è´¦æˆ·æ•°: {df['ä¼šå‘˜è´¦å·'].nunique():,}")
        st.write(f"ğŸ“… å”¯ä¸€æœŸå·æ•°: {df['æœŸå·'].nunique():,}")
        
        # å½©ç§åˆ†å¸ƒ
        lottery_stats = df['å½©ç§'].value_counts()
        st.write("ğŸ² å½©ç§åˆ†å¸ƒ:")
        for lottery, count in lottery_stats.items():
            st.write(f"  - {lottery}: {count:,} è¡Œ")
        
        st.success("âœ… ç‰¹ç åˆ†æå®Œæˆï¼ˆåŸºç¡€ç»Ÿè®¡ä¿¡æ¯ï¼‰")
        
    except Exception as e:
        st.error(f"âŒ ç‰¹ç åˆ†æå¤±è´¥: {str(e)}")

def main():
    """ä¸»å‡½æ•°"""
    st.title("ğŸ¯ æ™ºèƒ½å¤šè´¦æˆ·å¯¹åˆ·æ£€æµ‹ç³»ç»Ÿ")
    st.markdown("---")
    
    # ä¾§è¾¹æ é…ç½®
    st.sidebar.header("âš™ï¸ æ£€æµ‹å‚æ•°é…ç½®")
    
    min_amount = st.sidebar.number_input("æœ€å°æŠ•æ³¨é‡‘é¢", value=10, min_value=1)
    similarity_threshold = st.sidebar.slider("é‡‘é¢åŒ¹é…åº¦é˜ˆå€¼", 0.8, 1.0, 0.9, 0.01)
    min_continuous_periods = st.sidebar.number_input("æœ€å°è¿ç»­å¯¹åˆ·æœŸæ•°", value=3, min_value=1)
    
    # æ–‡ä»¶ä¸Šä¼ 
    st.header("ğŸ“ æ•°æ®ä¸Šä¼ ")
    uploaded_file = st.file_uploader(
        "è¯·ä¸Šä¼ æ•°æ®æ–‡ä»¶ (æ”¯æŒ .xlsx, .xls, .csv)", 
        type=['xlsx', 'xls', 'csv']
    )
    
    # åŠŸèƒ½å¼€å…³
    st.sidebar.header("ğŸ”§ åŠŸèƒ½å¼€å…³")
    enable_tema_analysis = st.sidebar.checkbox("å¯ç”¨ç‰¹ç åˆ†æ", value=False)
    
    if uploaded_file is not None:
        try:
            # æ›´æ–°é…ç½®
            config = Config()
            config.min_amount = min_amount
            config.amount_similarity_threshold = similarity_threshold
            config.min_continuous_periods = min_continuous_periods
            
            detector = WashTradeDetector(config)
            
            st.success(f"âœ… å·²ä¸Šä¼ æ–‡ä»¶: {uploaded_file.name}")
            
            with st.spinner("ğŸ”„ æ­£åœ¨è§£ææ•°æ®..."):
                df, filename = detector.upload_and_process(uploaded_file)
                if df is not None:
                    df_valid = detector.parse_column_data(df)
                    
                    if len(df_valid) > 0:
                        st.success("âœ… æ•°æ®è§£æå®Œæˆ")
                        
                        # å¯¹åˆ·æ£€æµ‹
                        st.info("ğŸš€ å¼€å§‹æ£€æµ‹å¯¹åˆ·äº¤æ˜“...")
                        with st.spinner("ğŸ” æ­£åœ¨æ£€æµ‹å¯¹åˆ·äº¤æ˜“..."):
                            patterns = detector.detect_all_wash_trades()
                        
                        if patterns:
                            st.success(f"âœ… æ£€æµ‹å®Œæˆï¼å‘ç° {len(patterns)} ä¸ªå¯¹åˆ·ç»„")
                            detector.display_detailed_results(patterns)
                            
                            # å¯¼å‡ºåŠŸèƒ½
                            excel_output, export_filename = detector.export_to_excel(patterns, filename)
                            if excel_output is not None:
                                st.download_button(
                                    label="ğŸ“¥ ä¸‹è½½æ£€æµ‹æŠ¥å‘Š",
                                    data=excel_output,
                                    file_name=export_filename,
                                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
                                )
                        else:
                            st.warning("âš ï¸ æœªå‘ç°ç¬¦åˆé˜ˆå€¼æ¡ä»¶çš„å¯¹åˆ·è¡Œä¸º")
                        
                        # ç‰¹ç åˆ†æ
                        if enable_tema_analysis:
                            run_simple_tema_analysis(df_valid)
                    
                    else:
                        st.error("âŒ æ•°æ®è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå†…å®¹")
            
        except Exception as e:
            st.error(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
    
    # ä½¿ç”¨è¯´æ˜
    with st.expander("ğŸ“– ä½¿ç”¨è¯´æ˜"):
        st.markdown("""
        ### ç³»ç»ŸåŠŸèƒ½è¯´æ˜

        **ğŸ¯ å¯¹åˆ·æ£€æµ‹ï¼š**
        - æ£€æµ‹2-3ä¸ªè´¦æˆ·ä¹‹é—´çš„å¯¹åˆ·è¡Œä¸º
        - æ”¯æŒå¯¹ç«‹æŠ•æ³¨ç±»å‹ï¼šå¤§ vs å°ã€å• vs åŒã€é¾™ vs è™
        - é‡‘é¢åŒ¹é…åº¦ â‰¥ 90%
        - æœ€å°è¿ç»­å¯¹åˆ·æœŸæ•°å¯é…ç½®

        **ğŸ“ æ•°æ®æ ¼å¼è¦æ±‚ï¼š**
        - æ–‡ä»¶å¿…é¡»åŒ…å«ï¼šä¼šå‘˜è´¦å·ã€æœŸå·ã€å†…å®¹ã€é‡‘é¢
        - å¯é€‰åŒ…å«ï¼šå½©ç§
        - æ”¯æŒExcelå’ŒCSVæ ¼å¼

        **ğŸ”§ ä½¿ç”¨æ­¥éª¤ï¼š**
        1. ä¸Šä¼ æ•°æ®æ–‡ä»¶
        2. è°ƒæ•´æ£€æµ‹å‚æ•°ï¼ˆå¯é€‰ï¼‰
        3. ç³»ç»Ÿè‡ªåŠ¨è§£ææ•°æ®å¹¶æ£€æµ‹
        4. æŸ¥çœ‹æ£€æµ‹ç»“æœå¹¶ä¸‹è½½æŠ¥å‘Š
        """)

if __name__ == "__main__":
    main()
